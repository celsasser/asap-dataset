---
title: "Analyzed Scores and Performances (ASAP)"
author: "Curtis Elsasser"
format:
  revealjs:
    css: "styles.css"
    embed-resources: true
editor_options: 
  chunk_output_type: console
---

```{r}
#| warning: false
source("wrangle.R")
library(kableExtra)

# note: load_manifest() sorts by `year_midlife`
list_manifest <- load_manifest()
list_scores <- load_music(list_manifest$scores)
half <- ceiling(length(list_scores) / 2)
tbl_music_all <- bind_rows(list_scores)
tbl_music_old <- bind_rows(list_scores[1:half])
tbl_music_new <- bind_rows(list_scores[(half + 1):length(list_scores)])
year_border <- tbl_music_new$year_written[1]
```


## Overview

Music is a mystery. Our ears love it and are quick to process it, but our brains struggle to understand it. For this reason, I am interested in looking at it from a different point of view. Why?

* I am a musician and I love music. 
* I am a budding data scientist and I love data.
* Music theory is not always satisfying.
* I want to look for patterns in music that a machine may be better at finding than us.
* Different perspectives nurture new discoveries.
* For a long time, I have wanted to examine music through the lens of a computer. What better time than now?

## The Data

::: {.font-size-three-quarters}
Aligned Scores and Performances (ASAP) dataset [repository](https://github.com/celsasser/asap-dataset). Fork of the ASAP dataset [repository](https://github.com/fosfrancesco/asap-dataset) whose authors are: Foscarin, Francesco and McLeod, Andrew and Rigaux, Philippe and Jacquemard, Florent and Sakai, Masahiko

Follows are the composers and their work represented in the catalog.
:::

::: {.metadata}
|Composers|Performances|Scores|
|---------|------------|------|
|**Bach**|169|59|
|**Balakirev**|10|1|
|**Beethoven**|271|57|
|**Brahms**|1|1|
|**Chopin**|289|34|
|**Debussy**|3|2|
|**Glinka**|2|1|
|**Haydn**|44|11|
|**Liszt**|121|16|
|**Mozart**|16|6|
|**Prokofiev**|8|1|
|**Rachmaninoff**|8|4|
|**Ravel**|22|4|
|**Schubert**|62|13|
|**Schumann**|28|10|
|**Scriabin**|13|2|
|**Total**|1067|222|
:::
:::


## Scores vs. Performances

Performances and scores are very closely related, but they are not the same. The score is the composition as written by the composer. The performance is the composition as played by the performer. Classical music is a very structured genre, but the performance of it is very expressive. It's very difficult to reproduce it in it's entirety with metadata such as dynamics. Where the tempo, key-signature and time-signature are meaningful in the score, they are meaningless in the performances in this repository. According to their metadata (in the MIDI files) they all look as if the were written in 4/4, the key of C and at 120/117 BPM. The score is the blueprint, the performance is the building.


## Schema: the Catalog
::: {.metadata}
| Column | Description | Type |
| --- | --- | --- |
| `id` | ID that identifies this composition | `string` |
| `composer` | Composer's last name | `string` |
| `year_born` | Composer's birth year (Wikipedia) | `integer` |
| `year_died` | Composer's death year (Wikipedia) | `integer` |
| `year_midlife` | Composer's halfway point (Wikipedia) | `integer` |
| `title` | Composition's title | `string` |
| `performer` | The performer. Extracted from `midi_performance` | `string` |
| `midi_score` | The composition's MIDI score file (relative path) | `string` |
| `midi_performance` | The composition's MIDI performance file (relative path) | `string` |
| `csv_score` | The score's CSV (relative path) | `string` |
| `csv_performance` | The performance's CSV (relative path) | `string` |
:::

## Schema: the Score/Performance
::: {.metadata}
| Column | Description | Type |
| --- | --- | --- |
| `id` |  composition ID | `integer` |
| `composer` | The composer of the piece | `string` |
| `year_born` | The composer's birth year (Wikipedia) | `integer` |
| `year_died` | The composer's death year (Wikipedia) | `integer` |
| `year_written` | This as an approximation that is accurate to half the composer's lifetime. | `integer` |
| `title` | The composition's title | `string` |
| `performer` | The performer. Extracted from `midi_performance`. `NA` for scores | `string`\|`NA` |
| `type` | The type of data. Music is all `note` | `string` |
| `time_offset` | The number of seconds from the beginning | `float` |
| `time_duration` | The duration in seconods | `float` |
| `tick_offset` | The number of MIDI ticks from the beginning | `integer` |
| `tick_duration` | The duration in MIDI ticks | `integer` |
| `note_midi` | The MIDI value of the note | `integer` |
| `note_normal` | The MIDI value normalized, [0, 1] | `integer` |
| `velocity` | The velocity of the note, [0, 1] | `integer` |
| `pretty` | The named representation of the note. Matches key-signature's spelling | `string` |
| `canonical` | The canonical representation of the note. We always use the flat equivalent | `string` |
| `density` | How dense notes are in the vicinity of this note. | `float` |
| `interval` | The interval between this note and the following note | `string` |
| `tempo` | The tempo of the current point in the piece | `integer` |
| `key_signature` | The key signature at this point in the piece | `string` |
| `time_signature` | The time signature at this point in the piece | `string` |
| `ticks_per_quarter` | The number of ticks in a quarter note | `integer` |
:::

## MIDI?
MIDI = Musical Instrument Digital Interface. 

:::: {.columns}
::: {.column width="25%"}
![](./res/robot-03.png){height=250px}
:::

::: {.column width="75%"}
|Type|Performer|Media|
|----|------|-----|
|Audio|Elaine Lee| ![](./res/bwv848-lee-live.mp3) |
|MIDI Performance|Elaine Lee| ![](./res/bwv848-lee-midi.mp3) |
|MIDI Score|NA| ![](./res/bwv848-score-midi.mp3) |

::: {.font-size-three-quarters}
The Well-Tempered Clavier I No. 3 in C-sharp major (BWV 848) by J.S. Bach
:::
:::
:::

## The Question

### Has note variance increased over time?

I believe it may have. I shall use an F-test to determine if there is a significant difference in note variance between the two portions of the dataset: 1685 - 1799 and 1799 - 1953.

**Independent variable**: time.

**Dependent variable**: note variance.

## Composers

```{r}
#| fig-width: 8
#| fig-height: 4
list_manifest$scores |>
  nest_by(composer, year_born, year_died) |>
  arrange(year_born) |>
  mutate(composer = factor(composer)) |>
  ggplot(mapping = aes(x = year_born, y = composer, color = composer)) +
  geom_segment(mapping = aes(xend = year_died), linewidth = 6, show.legend = FALSE) +
  geom_vline(xintercept = year_border, linetype = "dashed") +
  labs(
    title = "Composers and their Lifespans", 
    subtitle = "Dashed line represents the year that divides the composers into two groups.",
    x = "Lifetime", 
    y = "Composer"
  )
  
```

## Note Variance

```{r}
tbl_music_all |>
  ggplot(mapping = aes(x = note_normal, y = factor(year_written), )) +
  geom_boxplot() +
  labs(
    title = "IQR+ by Year",
    x = "Note Value",
    y = "~Year Written"
  )
```

## Assumptions

1. Is data is normally distributed?
```{r}
music_all_nn_mean <- mean(tbl_music_all$note_normal)
music_all_nn_sd <- sd(tbl_music_all$note_normal)

tbl_music_all |>
  ggplot(mapping = aes(x = note_normal)) +
  geom_histogram(
    mapping = aes(y = ..density..), 
    binwidth = 0.025
  ) +
  stat_function(
    args = list(mean = music_all_nn_mean, sd = music_all_nn_sd),
    fun = dnorm,
    mapping = aes(color = "red", linewidth = 0.025),
    show.legend = FALSE
  ) +
  labs(
    title = "Density Plot of Note Values",
    x = "Note Value", 
    y = "Density"
  )

```

## QQ Plot

```{r}
tbl_music_all |>
  ggplot(mapping = aes(sample = note_normal)) +
  geom_qq() +
  geom_qq_line() +
  labs(
    title = "QQ Plot",
    x = "Theoretical Quantiles",
    y = "Sample Quantiles"
  )
```

## Numbers
::: {.columns}
::: {.column width="50%"}
**1685 - 1799**

```{r}
tbl_music_old |>
  summarise(
    mean = mean(note_normal),
    sd = sd(note_normal),
    note_count = n()
  ) |>
  kable()
```
:::

::: {.column width="50%"}
**1799 - 1953**

```{r}
tbl_music_new |>
  summarise(
    mean = round(mean(note_normal), 3),
    SD = round(sd(note_normal), 3),
    note_count = n()
  ) |>
  kable()

```

:::
:::

## Assumptions (cont.)
1. Is data is normally distributed?

Yes, sufficiently enough for us to proceed.

2. Independence?

Yes, each note is independent of the other notes.

3. Homogeneity of variance?

We calculated our SD for both portions of our dataset and found that they are close, with a difference of ~0.01, which is 1% of our range. This is acceptable.

## Hypothesis

$H_0$: There is no significant difference in note variance over time.

$H_1$: There is a significant difference in note variance over time.


## F Test

```{r}
result <- var.test(tbl_music_old$note_normal, tbl_music_new$note_normal)
print(result)
```

## Conclusion

The p-value is crazy small which gives me reason to believe that I can reject the null hypothesis. There is a significant difference in note variance between the two portions of the dataset.

## Important?

It is important in the same way that history is important. It informs us of who we are and where we might be going. Interestingly, I would guess that the variance in note values has decreased in the past ~70 years. But this does not make us bad people. Variance in music does not correlate to quality. Egads, I wouldn't want to touch that experiment with a ten-foot pole.

## Limitations?

* Do the samples faithfully represent the periods between 1685 - 1953?
  * Not randomly selected. They are the big-shots. 
* Are the works included representative of their composers?
* My own limitations. I am still struggling somewhat with statistical tests.

## Naughty or Nice

The Well-Tempered Clavier I No. 3 in C-sharp major

It is the most impossible key in the whole of the Wohltemperirte Clavier: C-sharp major. No fewer than seven sharps adorn the beginning of each staff. Furthermore, it is an unnecessarily complicated key, as instead of seven sharps you could use five flats to write exactly the same pitch – as D-flat major. In 1728, the music theorist Johann David Heinichen therefore classified C-sharp major as one of the ‘superfluous keys’. Here, Bach is deliberately toying with the mind of the keyboard player, as the instinctive correspondence between the black noteheads on the paper and the fingers on the keys no longer works.

[Patrick Ayrton](https://www.bachvereniging.nl/en/bwv/bwv-848)
