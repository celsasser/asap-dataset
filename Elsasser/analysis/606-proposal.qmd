---
author: "Curtis Elsasser"
title: "DATA 606: Final Project Proposal"
subtitle: "Music, Stat! Version 0.1"
toc: false
editor: source
---

## History
I have made a proposal for a final project in DATA607 that begins the same way this one does and involves the same data preparation as this one does. So the first stage (dataset preparation) will be shared by DATA606 and DATA607. I don't think this is pulling a fast one, because the first stage is translating a dataset from an unconventional format to one that is more conventional in the world of data science. All will be explained below. I just wanted to make sure that it was known that part of the proposal and some the work will be shared between the two classes.

## Introduction
Music is a mystery. Our ears love it and are quick to process it, but our brains struggle to understand it. I am interested in looking at it from a different point of view. Why? For a number of reasons:

* I am a musician and I love music. 
* I am a budding data scientist and I love data. 
* Music theory is not always satisfying. It cannot be denied that it has a strong foundation in mathematics. But it also can be argued that it can be very subjective. And in contemporary times it seems like it has become a set of rules made to be broken.
* I want to see if I can find patterns in music that a machine may be better at finding than our ears.
* Music can be challenging to talk about. It would be interesting to create a new language with which talk about music.
* Different perspectives nurture different approaches. I hope that whatever discoveries lie within this project will help me write music in a new way.
* For a long time, I have wanted to "look" at music with through the lens of a computer. This seems like a great opportunity to apply the methods of probabiliies and statistics to music. 

As I write this, I find myself laughing and thinking, "Oh my gosh, that sounds drop dead boring!" But I think that's the point. "To make it boring?" No, I want to see if I can take music to another level. I think such a project has the potential to be fascinating. And to be honest, I am not totally sure how to do that. But one must start somewhere. What better place than here? What better time than now?

## Data
I would like to create a dataset from a music repository. The repository I would like to work with is called [ASAP](https://github.com/fosfrancesco/asap-dataset) and it is a collection of classical music performances. The repository has a registry as well as a collection of MIDI files that can be paired with [WAV files](https://magenta.tensorflow.org/datasets/maestro). For analysis, I am interested in the MIDI files because they are relatively small, easy to parse and are explicit in their representation of music. There are a total of 1,067 performances of compositions by 16 composers. And the composers bridge a wide range of time periods from the 17th century to the 21st century.

### Export
I plan to export the MIDI data as CSV files. There will be two types of CSV files. One will be a file of [compositions](#composition-metadata). It can be thought of as a registry. The other will be multiple [performance](#performance-data) files, one performance per file. The composition file will contain the metadata found in the [composition](#composition-metadata) schema. And the performance files will contain [performance](#performance-data) data as is described in their schema. 

### Import
I will be using the [readr](https://readr.tidyverse.org/) package to read the CSV files. And I will be using the join functions from the [dplyr](https://dplyr.tidyverse.org/) package to join composition and performance data. The performance data will be large. How much I load will certainly depend on the type of analysis I am doing, but may also be limited to how much my computer can handle. There is always the possibility of using a disk based database to store the data, but I don't imagine it will be necessary.

### Schema
My design approximates a 3rd normal form relational database. 

#### `composition` Metadata
The `composition` dataframe will be a file of compositions. It may be thought of as a registry of performances with some composition metadata. 

|Variable|Type|Attributes|Description|
|--|--|--|------|
|performance_id|integer|Primary key|Unique identifier for every performance|
|composition_id|integer| |Identifier for every composition|
|composer|character| |Name of the composer|
|year|integer| |Year the composition was written. It is not in ASAP, but imagine I can draw it in from elsewhere|
|performer|character| |Name of the performer. I think I can tease the last name out of the MIDI file name|
|title|character| |Title of the composition|
|midi_path|character| |Relative path to the MIDI file|
|length|double| |Length of the performance in seconds|
</br>

#### `performance` Data
The `performance` dataframe will be a file of performance data. Each performance will be a separate file.

|Variable|Type|Attributes|Description|
|--|---|---|-----|
|performance_id|integer|Foreign key|Identifier of `performance_id` in  [composition](#composition-metadata) table|
|key|[type.Key](#type-variable)||See `Key` in the [type](#type-variable) schema|
|value|[type.Value-Type](#type-variable)||See `Value` in the [type](#type-variable) schema. See the "The `value` Variable" warning below| 
|offset|double||Time offset in seconds|
|duration|double||Duration of the event in seconds|
</br>

:::{.callout-warning}
##### The `value` Variable?
When looking at the `Example` variable, one can see that the data is of mixed types. Encoding them all as strings is fine for their existence in a CSV file. But it is not ideal for statistical computations and, in R, it's not possible to have them in the same vector. I need to find a solution that works well in R.

1. I could spread it across multiple columns and parallel the MIDI file structure. This would be a lot of columns. I don't like this option. It's confusing and narrows the audience that would be willing to consume the specification. Plus, on the R side, we still have the same problem. We don't want to do data analysis over a bunch of enigmatic columns.
1. I could move the time signature and key signature to their own column.
1. Every note could carry the current time signature, key signature and maybe tempo. This has both pros and cons. The advantage is that we don't have to backtrack to find the time signature, key signature, tempo context that a particular note lives in. The principle con would be redundancy and bloating. The performance files will be large to begin with. I actually like this idea. It could be encoded as proposed above and transformed upon import to R.
1. I could encode the time signature and key signature as numbers. This would work for key signatures, but time signatures would be more challenging.
:::

##### `type` Variable
We have extracted this variable out of the `performance` schema so that we could document the various types that will live in here and give examples of those same types. This variable is technically two different variables: `Key` and `Value`. The `Key` variable will be a character string that identifies the type of data that is in the `Value` variable. We talk about ways to encode him and parse him below in the "The `value` Variable" warning above.

|Key|Value-Type|Value|Description|
|---|---|---|---|
|"ts"|string|"4/4"|Time signature|
|"ks"|string|"Eb Major"|Key signature|
|"tp"|double|120.0|Tempo|
|"nt"|integer|60|Note. The encoding is "MIDI note". I think it makes more sense to use notation conventions: C4, D4, etc. But I'm not loving the enharmonic spelling problem.|


##  Data analysis plan
The possibilities are exciting and endless. But, to be honest, I don't have them all ironed out yet. I am curious about some very basic statistics such as comparing composers and their compositions via their mean, standard deviation, note distribution and velocity distributions per performance. I am curious to see what the distributions will resemble (normal?) and, from those stats, what may be told about the compositions and perhaps the composers. And I am very curious about the time periods in which the compositions were written. As mentioned above, the repository spans a wide range of time periods (17th century to the 21st century). There are characteristics that typify each period. I know those characteristics are embedded in the data, I'm just don't know how to pull them out with probabilities and statistics.

A feature of the ASAP repository that I am particularly excited about is that for many compositions there are multiple performances by different artists. I am curious to see how such performances differ and whether there are patterns that can be found that explain the differences. I am also curious to see if there are patterns that can be found that explain the similarities.

I know this data analysis plan is vague. It was for DATA607 as well. I am submitting it today, but hope to have more clarity by the time the project proposal is due, which I learned today is 10/10. This morning I thought it was midnight tonight (10/3). I am grateful for the extra time, I shall use it wisely.
